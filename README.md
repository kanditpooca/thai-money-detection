## thai-money-detection

# Task 1 (Data Acquisition):
  We created our own dataset and used an online dataset to get our data. We could use the online dataset's wide variety of instances for both training and testing our models. We also took the initiative to develop our own dataset, which enabled us to collect particular data points and modify them to meet the needs of our project. Our models' robustness was guaranteed by the combination of these two sources, which gave us access to a large dataset that covered a wide range of scenarios.

# Task 2 (Data Preparation):
  We used Roboflow as a tool to speed up the annotation of our dataset because it gave us an effective labeling and annotation solution. We separated the dataset into three subgroups for training, validation, and testing after it had been prepared. To ensure a representative distribution throughout the subgroups, 87% of the data was allocated for training, 8% for validation, and 5% for testing. We used the "auto-orient" functionality to standardize the image orientation during the preprocessing stage. In order to improve data consistency during training, we also scaled the photos to a standard dimension. We used augmentations, such as 90-degree rotations (clockwise, counter-clockwise, and upside-down), to increase the dataset's diversity and resilience. In order to introduce variation in object angles, we also added a rotation range of -15 degrees to 15 degrees. These preprocessing and augmentation methods produced an extensive and varied dataset suitable for our models' training. 
  Finding a sufficient dataset to assure reliable results was a hurdle we ran into while working on our project. In Particular, we encountered problems with coin detection, which caused mistakes in our preliminary models. We took the required measures to develop an enlarged dataset that is exclusively targeted at coins in order to overcome this problem and enhance the precision of our coin detection. We were able to improve the training data and fix the detection problems by collecting more photos and annotations of different currencies. Our coin detection system's overall effectiveness and accuracy were greatly enhanced by the iterative process of expanding the dataset size and upgrading the models.

# Task 3 (Model Training and deploy):
  In order to guarantee a simplified and segregated workspace for our model training, our project's adventure started with the creation of a separate environment utilizing Anaconda. After setting up the environment, we started training our model by entering the required commands into the terminal. The model's performance was improved by numerous iterations during the training phase, which entailed running the training script 100 times in total. During this demanding training session, which lasted about 1.3 hours, we refined the model's accuracy and adjusted its parameters.
  The lack of a diversified dataset for coin detection was one of the difficulties we faced during the training phase. The visual variations between the two sides of a coin were frequently modest, making it difficult to tell which side was the head. The surfaces of the 1 and 5 baht coins also shared similarities, making it more difficult to distinguish between the two denominations. We worked hard to increase our dataset specifically for coins in order to get around this obstacle. We were able to solve the problem and greatly raise the accuracy of our currency detection algorithm by gathering a wider variety of coin photos and painstakingly annotating them.
  We ran into some difficulties because of our lack of knowledge with the Jetson platform when we worked to deploy our resources on the Jetson Nano. It took a while to correctly import the essential libraries into the Jetson Nano environment; some libraries took up to 4 hours to import. We had to practice patience and look at other options in order to optimize the library importation process because of this unforeseen delay.
We also had trouble getting a reliable internet connection to work on the Jetson Nano board. Despite our initial efforts, we had trouble connecting over Ethernet, and the connection would occasionally drop. We used a LAN wire to establish a constant and uninterrupted connection between the router and the Jetson Nano board after exhausting all other options in our effort to fix this problem.

# Task 4 (Model Inference):
  In order to create our dataset, we started the data annotation process by taking pictures. We took great care to make sure that each image faithfully portrayed the desired annotations. In order to watch and evaluate the performance of our datasets in action, we recorded films during the testing and training phases of our data analysis. For the purpose of improving and assessing our models, the movies were an invaluable resource. In addition, we used the strength of visual storytelling by producing more films to advertise our work on websites like YouTube. These movies demonstrated the capabilities of our dataset and model, proving their efficacy and piqued the audience's attention. We reduced the highlights of our YouTube films into compelling shorts to increase reach and engagement.
